{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cpatiffanynguyen/usda/blob/main/webscraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMgMZtUAvqWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8d4b965-d92a-43ba-905c-f244d389edb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of centers: 200\n",
            "Mounted at /content/drive\n",
            "File written successfully to /content/drive/My Drive/testing.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "from google.colab import drive\n",
        "import re\n",
        "import os\n",
        "\n",
        "extension_centers = []\n",
        "\n",
        "def fetch_and_parse_html(link):\n",
        "    try:\n",
        "        response = requests.get(link)\n",
        "        if response.status_code == 200:\n",
        "            return BeautifulSoup(response.content, 'html.parser')\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching {link}: {e}\")\n",
        "    return None\n",
        "\n",
        "def get_lat_lng_add(address):\n",
        "    endpoint = f\"https://nominatim.openstreetmap.org/search?q={address}&format=json\"\n",
        "    r = requests.get(endpoint)\n",
        "    if r.status_code not in range(200, 299):\n",
        "        print(f\"Failed to get data: Status code {r.status_code}\")\n",
        "        return\n",
        "    try:\n",
        "        results = r.json()\n",
        "        if not results:\n",
        "            print(\"No results found\")\n",
        "            return\n",
        "        return results[0]['lat'], results[0]['lon']\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return\n",
        "\n",
        "def get_lat_lng_from_zip(zip_code):\n",
        "    endpoint = f\"https://nominatim.openstreetmap.org/search?q={zip_code}&format=json\"\n",
        "    try:\n",
        "        r = requests.get(endpoint)\n",
        "        if r.status_code == 200:\n",
        "            results = r.json()\n",
        "            if results:\n",
        "                latitude = results[0].get('lat')\n",
        "                longitude = results[0].get('lon')\n",
        "                if latitude and longitude:\n",
        "                    return latitude, longitude\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "    return None, None  # Return None for both latitude and longitude if unavailable\n",
        "\n",
        "def get_lat_lng(zip_or_address):\n",
        "    \"\"\"Try to fetch latitude and longitude based on either zip code or address.\"\"\"\n",
        "    base_url = \"https://nominatim.openstreetmap.org/search\"\n",
        "    params = {\n",
        "        'q': zip_or_address,\n",
        "        'format': 'json'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            results = response.json()\n",
        "            if results:\n",
        "                return results[0]['lat'], results[0]['lon']\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching coordinates for {zip_or_address}: {e}\")\n",
        "    return None, None\n",
        "\n",
        "def fetch_coordinates_for_center(address, zip_code):\n",
        "    \"\"\"Attempts to fetch coordinates first using the zip code, then the address.\"\"\"\n",
        "    lat, lng = get_lat_lng(zip_code)\n",
        "    if lat and lng:\n",
        "        return lat, lng\n",
        "    return get_lat_lng(address)  # Try with address if zip code fails\n",
        "\n",
        "\n",
        "\n",
        "def Alabama(link):\n",
        "    soup = fetch_and_parse_html(link)\n",
        "    if soup:\n",
        "        county_links = [link + county_link['href'] for county_link in soup.find_all('a', class_='btn btn-default btn-county')]\n",
        "        for county_link in county_links:\n",
        "            county_soup = fetch_and_parse_html(county_link)\n",
        "            if county_soup:\n",
        "                address_tag = county_soup.find('strong', string='Address:')\n",
        "                canonical_link = county_soup.find('link', rel='canonical')\n",
        "                name = canonical_link['href'].rstrip('/').split('/')[-1].capitalize()\n",
        "                address = ''\n",
        "                if address_tag:\n",
        "                    address_lines = []\n",
        "                    sibling = address_tag.next_sibling\n",
        "                    while sibling and len(address_lines) < 3:\n",
        "                        if sibling.string:\n",
        "                            address_lines.append(sibling.string.strip())\n",
        "                        sibling = sibling.next_sibling\n",
        "                    match = re.search(r'(.+?\\b\\d{5}\\b)', ' '.join(address_lines))\n",
        "                    if match:\n",
        "                        address = match.group(1).strip()\n",
        "                    else:\n",
        "                        address = \"No valid address found\"\n",
        "                lat, lng = get_lat_lng_from_zip(address[-5:])\n",
        "                if lat and lng:\n",
        "                    extension_centers.append((name, address, address[-5:], 'Alabama', lat, lng))\n",
        "                else:\n",
        "                    print(f\"Failed to get lat/lng for address: {address}\")\n",
        "    else:\n",
        "        print(\"Alabama web page error\")\n",
        "\n",
        "\n",
        "def Alaska(link):\n",
        "    soup = fetch_and_parse_html(link)\n",
        "    if soup:\n",
        "        location_links = soup.find_all('a', href=lambda href: href and \"ces/districts\" in href)\n",
        "\n",
        "        for i in range(len(location_links)):\n",
        "            if i <= 1 or i >= len(location_links) - 2:\n",
        "                continue\n",
        "            location_link = location_links[i]\n",
        "            location_name = location_link.text.strip()\n",
        "            location_url = link[:-1] + location_link['href'][14:]\n",
        "\n",
        "            location_page_soup = fetch_and_parse_html(location_url)\n",
        "            if location_page_soup:\n",
        "                address_tags = location_page_soup.find_all(\n",
        "                    'a',\n",
        "                    href=lambda href: href and (\"goo.gl/maps\" in href or \"google.com/maps\" in href) if href else False\n",
        "                )\n",
        "\n",
        "                for address_tag in address_tags:\n",
        "                    full_address = ''\n",
        "                    current_element = address_tag\n",
        "\n",
        "                    while current_element:\n",
        "                        if current_element.name == 'a' and (current_element['href'].startswith(\"https://goo.gl/maps\") or current_element['href'].startswith(\"https://www.google.com/maps\")):\n",
        "                            full_address += current_element.get_text(strip=True) + ' '\n",
        "                        current_element = current_element.find_next_sibling()\n",
        "                        if current_element and current_element.name not in ['a', 'br']:\n",
        "                            break\n",
        "\n",
        "                    if full_address:\n",
        "                        full_address = full_address.strip()\n",
        "                        zip_code_pattern = re.compile(r'\\b\\d{5}(?:-\\d{4})?\\b')\n",
        "                        zip_code_match = zip_code_pattern.search(full_address)\n",
        "                        zip_code = zip_code_match.group() if zip_code_match else \"Zip code not found\"\n",
        "                        extension_centers.append((location_name, full_address, zip_code, 'Alaska', get_lat_lng_from_zip(zip_code)[0], get_lat_lng_from_zip(get_lat_lng_from_zip)[1]))\n",
        "                        break\n",
        "\n",
        "    return extension_centers\n",
        "\n",
        "\n",
        "def Connecticut(link):\n",
        "    soup = fetch_and_parse_html(link)\n",
        "    if soup:\n",
        "        location_links = soup.find_all('a', href=lambda href: href and \"https://cahnr.uconn.edu/extension/locations/\" in href)\n",
        "\n",
        "        for i, location_link in enumerate(location_links):\n",
        "            if i == 0 or i == len(location_links) - 1:\n",
        "                continue\n",
        "            location_name = location_link.text.strip()\n",
        "            location_url = location_link['href']  # Extract the URL\n",
        "\n",
        "            # Fetch and parse the individual location page\n",
        "            location_page_soup = fetch_and_parse_html(location_url)\n",
        "            if location_page_soup:\n",
        "                address_parts = location_page_soup.find_all('p', style=\"text-align: left;\")\n",
        "                full_address = ''\n",
        "                zip_code = ''\n",
        "                zip_code_pattern = re.compile(r'\\b\\d{5}(?:-\\d{4})?\\b')\n",
        "\n",
        "                for part in address_parts:\n",
        "                    text = part.get_text(strip=True)\n",
        "                    full_address += text + ' '\n",
        "                    # Check if the text contains a zip code\n",
        "                    zip_code_match = zip_code_pattern.search(text)\n",
        "                    if zip_code_match:\n",
        "                        zip_code = zip_code_match.group()  # Extract the zip code\n",
        "                        break  # Stop after finding the zip code\n",
        "                address = full_address.strip()\n",
        "                if not address:\n",
        "                    address = \"No address found\"\n",
        "                if not zip_code:\n",
        "                    zip_code = \"No zip code found\"\n",
        "            else:\n",
        "                address = \"Address not found\"\n",
        "\n",
        "            extension_centers.append((location_name, address, zip_code, 'Connecticut', get_lat_lng_from_zip(zip_code)[0], get_lat_lng_from_zip(zip_code)[1]))\n",
        "\n",
        "def Arizona(link):\n",
        "    return\n",
        "\n",
        "def Arkansas(link):\n",
        "    soup = fetch_and_parse_html(link)\n",
        "    if soup:\n",
        "        county_links = [(curr_url.get_text(), link + curr_url['href'][10:]) for col_class in [\"col1\", \"col2\", \"col3\", \"col4\"] if soup.find(class_=col_class) for curr_url in soup.find(class_=col_class).find_all('a')]\n",
        "        print(county_links)\n",
        "\n",
        "def California(link):\n",
        "    return\n",
        "\n",
        "def Colorado(link):\n",
        "    return\n",
        "\n",
        "\n",
        "state_functions = {\n",
        "    'https://ssl.acesag.auburn.edu/directory-new/': Alabama,\n",
        "    'https://www.uaf.edu/ces/districts/': Alaska,\n",
        "    'https://cahnr.uconn.edu/extension/locations/': Connecticut,\n",
        "    # 'https://www.uaex.uada.edu/counties/' : Arkansas\n",
        "}\n",
        "\n",
        "def process_link(link):\n",
        "    local_centers = []\n",
        "    if not pd.isna(link) and link in state_functions:\n",
        "        local_centers = state_functions[link](link)\n",
        "    return local_centers\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    csv_url = 'https://docs.google.com/spreadsheets/d/18p6w3btY2km9nEKq0Z8W0BAbHh8L5Z3k/export?format=csv&gid=926683666'\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(csv_url)\n",
        "        extension_links = df['Local Office Directory']\n",
        "\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            results = executor.map(process_link, extension_links)\n",
        "\n",
        "        for result in results:\n",
        "            if result is not None:\n",
        "                extension_centers.extend(result)\n",
        "\n",
        "        print(\"Number of centers:\", len(extension_centers))\n",
        "\n",
        "        if extension_centers:\n",
        "            df_extension_centers = pd.DataFrame(extension_centers, columns=['county', 'address', 'zipcode', 'state', 'latitude', 'longitude'])\n",
        "\n",
        "            # Update latitude and longitude where missing\n",
        "            for idx, row in df_extension_centers[df_extension_centers['latitude'].isnull() | df_extension_centers['longitude'].isnull()].iterrows():\n",
        "                lat, lng = fetch_coordinates_for_center(row['address'], row['zipcode'])\n",
        "                df_extension_centers.at[idx, 'latitude'] = lat\n",
        "                df_extension_centers.at[idx, 'longitude'] = lng\n",
        "\n",
        "            df_extension_centers = df_extension_centers.drop_duplicates(subset=['address', 'zipcode', 'state'])\n",
        "            df_extension_centers = df_extension_centers.sort_values(by='state', ascending=True)\n",
        "\n",
        "            drive.mount('/content/drive', force_remount=True)\n",
        "            path = '/content/drive/My Drive/testing.csv'\n",
        "\n",
        "            df_extension_centers.to_csv(path, index=False)\n",
        "            print(f\"File written successfully to {path}\")\n",
        "        else:\n",
        "            print(\"No extension centers to write.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}